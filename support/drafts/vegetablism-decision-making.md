# 菜学：决策之道

## 零、阅读须知

咳咳，所谓技多不压身，虽然我是时间管理大师，但不瞒大家说，我还是决策大师。算了，这篇写得严肃一点比较好，太活泼了你们光看不走心。

23岁的人自诩决策大师，是不是有点太自以为是了？对。这个年纪的小屁孩，能有多少阅历？有一句老话说得好：“我吃过的盐比你吃过的饭都多；我走过的桥比你走过的路都多。”所以，23岁的人充其量只是一个理论大师，一摊上真事就现原形。对。我这套理论，只用过两次，第一次是考研选校选专业；第二次是确定所谓的价值点。前者已经彻底成功了；后者十年之后见分晓。

那么，写这篇文章的动机是什么呢？教别人决策吗？不是。**我只是向大家提供一个可证实的、逻辑完备的理论最优解。信不信、信多少，都取决于你。**毕竟，我这无处安放的智慧，不写出来于我的发量有害。我一直很好奇：为什么老天爷要给我与生理年龄极度不匹配的智慧？我不堪其扰！

**所谓的可证实**：决策之道建立在概率论之上，绝不会出现自然语言左右互搏的闹剧。比如：“一份耕耘，一份收获”，“努力有用的话，还要天才干什么”，“时来天地皆同力，运去英雄不自由”，“天生我材必有用”，“活着浪费空气，死了浪费土地”。

**所谓的逻辑完备**：除了目标函数最大化之外，还考虑知足常乐、失之坦然等情况，保证了决策之道的多样性。

**所谓的理论最优解**：碍于碳基生物的局限性，完备的先验知识和绝对理性人是不存在的。但是，人因缺憾而美丽。

愿本文助你：**远离平庸，走上平凡之路。**

## 一、名词解释

### 1. 数学期望 

在[概率论](https://zh.wikipedia.org/wiki/概率论)和[统计学](https://zh.wikipedia.org/wiki/统计学)中，一个离散性[随机变量](https://zh.wikipedia.org/wiki/随机变量)的**期望值**（或**数学期望**，亦简称**期望**，物理学中称为**期待值**）是试验中每次可能的结果乘以其结果概率的[总和](https://zh.wikipedia.org/wiki/總和)。换句话说，期望值像是随机试验在同样的机会下重复多次，所有那些可能状态平均的结果，便基本上等同“期望值”所期望的数。期望值可能与每一个结果都不相等。换句话说，期望值是该变量输出值的加权平均。期望值并不一定包含于其分布值域，也并不一定等于值域平均值。

例如，掷一枚公平的六面[骰子](https://zh.wikipedia.org/wiki/骰子)，其每次“点数”的期望值是3.5，计算如下：

E⁡(X)=1⋅16+2⋅16+3⋅16+4⋅16+5⋅16+6⋅16=1+2+3+4+5+66=3.5 

不过如上所说明的，3.5虽是“点数”的期望值，但却不属于可能结果中的任一个，没有可能掷出此点数。

[赌博](https://zh.wikipedia.org/wiki/赌博)是期望值的一种常见应用。例如，[美国](https://zh.wikipedia.org/wiki/美国)的[轮盘](https://zh.wikipedia.org/wiki/轮盘)中常用的轮盘上有38个数字，每一个数字被选中的概率都是相等的。赌注一般押在其中某一个数字上，如果轮盘的输出值和这个数字相等，那么下赌者可以获得相当于赌注35倍的奖金（原注不包含在内），若输出值和下压数字不同，则赌注就输掉了。考虑到38种所有的可能结果，然后这里我们的设定的期望目标是“赢钱”，则因此，讨论赢或输两种预想状态的话，以1美元赌注押一个数字上，则获利的期望值为：赢的“概率38分之1，能获得35元”，加上“输1元的情况37种”，结果约等于-0.0526美元。也就是说，平均起来每赌1美元就会输掉0.0526美元，即美式轮盘以1美元作赌注的期望值为**负0.0526美元**。

E⁡(X)=35⋅138−1⋅3738≈−0.0526 

### 2. 最大似然估计 

给定一个概率分布 D ，已知其[概率密度函数](https://zh.wikipedia.org/wiki/概率密度函数)（连续分布）或[概率质量函数](https://zh.wikipedia.org/wiki/概率质量函数)（离散分布）为 fD ，以及一个分布参数 θ ，我们可以从这个分布中抽出一个具有 n 个值的采样 X1,X2,…,Xn ，利用 fD 计算出其[似然函数](https://zh.wikipedia.org/wiki/似然函数)：

L(θ∣x1,…,xn)=fθ(x1,…,xn) 

若 D 是离散分布， fθ 即是在参数为 θ 时观测到这一采样的概率。若其是连续分布， fθ 则为 X1,X2,…,Xn 联合分布的概率密度函数在观测值处的取值。一旦我们获得 X1,X2,…,Xn ，我们就能求得一个关于 θ 的估计。最大似然估计会寻找关于 θ 的最可能的值（即，在所有可能的 θ 取值中，寻找一个值使这个采样的“可能性”最大化）。从数学上来说，我们可以在 θ 的所有可能取值中寻找一个值使得似然[函数](https://zh.wikipedia.org/wiki/函数)取到最大值。这个使可能性最大的 θ^ 值即称为 θ 的**最大似然估计**。由定义，最大似然估计是样本的函数。

### 3. 参数估计 和置信度 

作为[统计学](https://zh.wikipedia.org/wiki/统计学)和[信号处理](https://zh.wikipedia.org/wiki/信号处理)中的一个分支，参数估计主要是通过测量或经验数据来估计[概率分布](https://zh.wikipedia.org/wiki/概率分布)[参数](https://zh.wikipedia.org/wiki/参数)的数值。这些参数描述了实质情况或实际对象，它们能够回答[估计函数](https://zh.wikipedia.org/wiki/估计函数)提出的问题。

例如，估计投票人总体中，给特定候选人投票的人的比例。这个比例是一个不可观测的参数，因为投票人总体很大；估计值建立在投票者的一个小的随机采样上。

又如，雷达的目的是物体（飞机、船等）的定位。这种定位是通过分析收到的回声（回波）来实现的，定位提出的问题是“飞机在哪里？”为了回答这个问题，必须估计飞机到雷达之间的距离。如果雷达的绝对位置是已知的，那么飞机的绝对位置也是可以确定的。

在估计理论中，通常假定信息隐藏在包含[噪声](https://zh.wikipedia.org/wiki/雜訊_(通訊學))的[信号](https://zh.wikipedia.org/wiki/信号)中。噪声增加了[不确定性](https://zh.wikipedia.org/wiki/不确定性)，如果没有不确定性，那么也就没有必要估计了。

在统计学中，一个概率样本的置信区间，是对产生这个样本的总体的参数分布中的某一个未知参数值，以区间形式给出的估计。相对于点估计用一个样本统计量来估计参数值，置信区间还蕴含了估计的精确度的信息。在现代机器学习中越来越常用的置信集合概念是置信区间在多维分析的推广。

置信区间在频率学派中间使用，其在贝叶斯统计中的对应概念是可信区间。两者建立在不同的概念基础上的，贝叶斯统计将分布的位置参数视为随机变量，并对给定观测到的数据之后未知参数的后验分布进行描述，故无论对随机样本还是已观测数据，构造出来的可信区间，其可信水平都是一个合法的概率；而置信区间的置信度，只在考虑随机样本时可以被理解为一个概率。

### 4. 大数定律与均值回归

### 5. 先验与后验 

**先验**（拉丁语：**A priori**，指“来自较早的”）和**后验**（拉丁语：**a posteriori**，指“来自较晚的”），是[哲学](https://zh.wikipedia.org/wiki/哲學)中使用的[拉丁语](https://zh.wikipedia.org/wiki/拉丁語)短语，用于透过对[经验](https://zh.wikipedia.org/wiki/經驗)证据或经验的依赖来区分[知识](https://zh.wikipedia.org/wiki/知識)、[证明](https://zh.wikipedia.org/wiki/證明)或[论证](https://zh.wikipedia.org/wiki/论证)的类型。先验知识是独立于经验的知识。例子包括[数学](https://zh.wikipedia.org/wiki/數學)、[重言式](https://zh.wikipedia.org/wiki/重言式)和出自纯粹理性的[演绎推理](https://zh.wikipedia.org/wiki/演绎推理)。后验知识是依赖于经验证据的知识。例子包括大多数[科学](https://zh.wikipedia.org/wiki/科學)领域和个人知识的各个方面。

### 6. 全概率公式 与贝叶斯公式 

假设{ *B* *n*  : *n* = 1, 2, 3, ... } 是一个[概率空间](https://zh.wikipedia.org/wiki/概率空間)的有限或者可数无限的[分割](https://zh.wikipedia.org/wiki/集合划分)（既*B* *n*为一完备事件组），且每个集合*B* *n*是一个[可测集合](https://zh.wikipedia.org/wiki/可测集合)，则对任意事件*A*有**全概率公式**：

Pr(A)=∑nPr(A∩Bn) 

又因为 Pr(A∩Bn)=Pr(A∣Bn)Pr(Bn), 

此处Pr( *A* | *B* )是*B*发生后*A*的[条件概率](https://zh.wikipedia.org/wiki/条件概率)，所以**全概率公式**又可写作：

Pr(A)=∑nPr(A∣Bn)Pr(Bn) 

全概率公式将对一复杂事件A的概率求解问题转化为了在不同情况或不同原因*B* *n*下发生的简单事件的概率的求和问题。

贝氏定理是关于随机事件A和B的[条件机率](https://zh.wikipedia.org/wiki/条件概率)的一则定理。

P(A∣B)=P(A)P(B∣A)P(B) 

其中 A 以及 B 为[随机事件](https://zh.wikipedia.org/wiki/随机事件)，且 P(B) 不为零。 P(A∣B) 是指在事件 B 发生的情况下事件 A 发生的机率。

在贝氏定理中，每个名词都有约定俗成的名称：

- P(A∣B) 是已知 B 发生后， A 的[条件机率](https://zh.wikipedia.org/wiki/条件概率)。也称作 A 的[事后机率](https://zh.wikipedia.org/wiki/后验概率)。
- P(A) 是 A 的[先验概率](https://zh.wikipedia.org/wiki/先验概率)（或[边缘机率](https://zh.wikipedia.org/wiki/边缘概率)）。其不考虑任何 B 方面的因素。
- P(B∣A) 是已知 A 发生后， B 的条件机率。也可称为 B 的后验概率。某些文献又称其为在特定 B 时， A 的[概似性](https://zh.wikipedia.org/wiki/似然函数)，因为 P(B∣A)=L(A∣B) 。
- P(B) 是 B 的[先验概率](https://zh.wikipedia.org/wiki/先验概率)。

按这些术语，贝氏定理可表述为：后验概率= (概似性*先验概率)/标准化常数。也就是说，后验概率与先验概率和相似度的乘积成正比。

另外，比例 P(B|A)/P(B) 也有时被称作标准概似度（standardised likelihood），贝氏定理可表述为：后验概率 = 标准概似度 * 先验概率。由贝叶斯公式：

P(θ|X)=P(θ)P(X|θ)P(X)∝P(θ)P(X|θ) 

这也是贝氏估计和极大概似估计的区别所在，极大概似估计中要估计的参数是个一般变量，而贝氏估计中要估计的参数是个随机变量。

## 二、方法论

### 1. 目标函数

乞丐版：加权平均数

正白 乌哈拉萨虎爵 赏都统 世袭佐领 兼云骑尉 噶图灰达拉哈多罗贝勒  VIP版：多元函数

### 2. 迭代

乞丐版：主因素分析

不焚者 风暴降生丹妮莉丝 弥林的女王 安达尔人 洛伊拿人和先民的女王 七国统治者暨全境守护者 大草海的卡丽熙 奴隶解放者  VIP版：Bayes归因法

## 三、增补

### 1. 底线思维

### 2. 失败的必然和偶然

### 3. 平凡的主观能动性

### 4. 生涯之学即迭代之学

### 5. 生涯之学即应变之学

### 6. 诗意藏在理性的尽头

### 7. 你不想和我一起建设社会主义核心价值观嘛？

> 你站在桥上看风景， 看风景人在楼上看你。 明月装饰了你的窗子， 你装饰了别人的梦。 ——《断章》  卞之琳

## 四、从平庸到平凡的第一次飞跃

## 五、有一种人生叫“骆驼祥子” 